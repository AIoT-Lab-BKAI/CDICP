{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-0.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-1.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-2.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-3.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-4.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-5.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-6.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-7.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-8.csv\n",
      "Loaded file: ./data/distributed/hailfinder/m3_d5_n10/silo-9.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "\n",
    "dataname = \"hailfinder\"\n",
    "\n",
    "mi = 3      # The number of values a variable can take is ranged in [2, mi-1]\n",
    "di = 5      # The dirichlet alpha that controls the data distribution\n",
    "n = 10      # The number of data silos\n",
    "\n",
    "silos = []\n",
    "\n",
    "folderpath = f\"./data/distributed/{dataname}/m{mi}_d{di}_n{n}\"\n",
    "groundtruth = np.loadtxt(f\"./data/distributed/{dataname}/adj.txt\", delimiter=' ')\n",
    "\n",
    "if not Path(folderpath).exists():\n",
    "    print(\"Folder\", folderpath, \"not exist!\")\n",
    "else:\n",
    "    for file in sorted(os.listdir(folderpath)):\n",
    "        filename = os.path.join(folderpath, file)\n",
    "        silo_data = pd.read_csv(filename)\n",
    "        silos.append(silo_data)\n",
    "        print(\"Loaded file:\", filename, end=\"\\n\")   \n",
    "             \n",
    "merged_df = pd.concat(silos, axis=0)\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns, key=lambda item: int(item[1:])), axis=1)\n",
    "all_vars = merged_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraint-based PC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf72022239df4f379238d7607af56e1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbnlearn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mbn\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructure_learning\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmerged_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethodtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m adj_mtx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;28mlen\u001b[39m(all_vars), \u001b[38;5;28mlen\u001b[39m(all_vars)])\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m edge \u001b[38;5;129;01min\u001b[39;00m model[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdag_edges\u001b[39m\u001b[38;5;124m'\u001b[39m]:     \u001b[38;5;66;03m# type:ignore\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bnlearn/structure_learning.py:206\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(df, methodtype, scoretype, black_list, white_list, bw_list_method, max_indegree, tabu_length, epsilon, max_iter, root_node, class_node, fixed_edges, return_all_dags, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcs\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconstraintsearch\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    201\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Constraint-based Structure Learning\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m    A different, but quite straightforward approach to build a DAG from data is this:\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m    Identify independencies in the data set using hypothesis tests\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m    Construct DAG (pattern) according to identified independencies (Conditional) Independence Tests\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m    Independencies in the data can be identified using chi2 conditional independence tests.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 206\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m_constraintsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mn_jobs\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mverbose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# TreeSearch-based Structure Learning\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchow-liu\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtan\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/bnlearn/structure_learning.py:393\u001b[0m, in \u001b[0;36m_constraintsearch\u001b[0;34m(df, significance_level, n_jobs, verbose)\u001b[0m\n\u001b[1;32m    390\u001b[0m model \u001b[38;5;241m=\u001b[39m ConstraintBasedEstimator(df)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Estimate using chi2\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m skel, seperating_sets \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_skeleton\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignificance_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignificance_level\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUndirected edges: \u001b[39m\u001b[38;5;124m\"\u001b[39m, skel\u001b[38;5;241m.\u001b[39medges())\n\u001b[1;32m    396\u001b[0m pdag \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mskeleton_to_pdag(skel, seperating_sets)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pgmpy/estimators/PC.py:304\u001b[0m, in \u001b[0;36mPC.build_skeleton\u001b[0;34m(self, ci_test, max_cond_vars, significance_level, variant, n_jobs, show_progress, **kwargs)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m u, v \u001b[38;5;129;01min\u001b[39;00m graph\u001b[38;5;241m.\u001b[39medges():\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m separating_set \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    299\u001b[0m         combinations(\u001b[38;5;28mset\u001b[39m(neighbors[u]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m([v]), lim_neighbors),\n\u001b[1;32m    300\u001b[0m         combinations(\u001b[38;5;28mset\u001b[39m(neighbors[v]) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m([u]), lim_neighbors),\n\u001b[1;32m    301\u001b[0m     ):\n\u001b[1;32m    302\u001b[0m         \u001b[38;5;66;03m# If a conditioning set exists remove the edge, store the\u001b[39;00m\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;66;03m# separating set and move on to finding conditioning set for next edge.\u001b[39;00m\n\u001b[0;32m--> 304\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mci_test\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m            \u001b[49m\u001b[43mu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m            \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m            \u001b[49m\u001b[43mseparating_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[43m            \u001b[49m\u001b[43mindependencies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindependencies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[43m            \u001b[49m\u001b[43msignificance_level\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignificance_level\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    311\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    312\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    313\u001b[0m             separating_sets[\u001b[38;5;28mfrozenset\u001b[39m((u, v))] \u001b[38;5;241m=\u001b[39m separating_set\n\u001b[1;32m    314\u001b[0m             graph\u001b[38;5;241m.\u001b[39mremove_edge(u, v)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pgmpy/estimators/CITests.py:94\u001b[0m, in \u001b[0;36mchi_square\u001b[0;34m(X, Y, Z, data, boolean, **kwargs)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchi_square\u001b[39m(X, Y, Z, data, boolean\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Chi-square conditional independence test.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m    Tests the null hypothesis that X is independent from Y given Zs.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpower_divergence\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mZ\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboolean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboolean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpearson\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pgmpy/estimators/CITests.py:550\u001b[0m, in \u001b[0;36mpower_divergence\u001b[0;34m(X, Y, Z, data, boolean, lambda_, **kwargs)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m z_state, df \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mgroupby(Z):\n\u001b[1;32m    548\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    549\u001b[0m         c, _, d, _ \u001b[38;5;241m=\u001b[39m stats\u001b[38;5;241m.\u001b[39mchi2_contingency(\n\u001b[0;32m--> 550\u001b[0m             \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39munstack(Y, fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), lambda_\u001b[38;5;241m=\u001b[39mlambda_\n\u001b[1;32m    551\u001b[0m         )\n\u001b[1;32m    552\u001b[0m         chi \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m c\n\u001b[1;32m    553\u001b[0m         dof \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m d\n",
      "File \u001b[0;32m~/miniconda3/envs/easyFL/lib/python3.8/site-packages/pandas/core/groupby/groupby.py:2420\u001b[0m, in \u001b[0;36mGroupBy.size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2407\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m   2408\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2409\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_common_see_also)\n\u001b[1;32m   2410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msize\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   2411\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2412\u001b[0m \u001b[38;5;124;03m    Compute group sizes.\u001b[39;00m\n\u001b[1;32m   2413\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[38;5;124;03m        or a DataFrame if as_index is False.\u001b[39;00m\n\u001b[1;32m   2419\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2420\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2422\u001b[0m     \u001b[38;5;66;03m# GH28330 preserve subclassed Series/DataFrames through calls\u001b[39;00m\n\u001b[1;32m   2423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, Series):\n",
      "File \u001b[0;32m~/miniconda3/envs/easyFL/lib/python3.8/site-packages/pandas/core/groupby/ops.py:918\u001b[0m, in \u001b[0;36mBaseGrouper.size\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     out \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 918\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Series(out, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult_index\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/easyFL/lib/python3.8/site-packages/pandas/_libs/properties.pyx:36\u001b[0m, in \u001b[0;36mpandas._libs.properties.CachedProperty.__get__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/easyFL/lib/python3.8/site-packages/pandas/core/groupby/ops.py:997\u001b[0m, in \u001b[0;36mBaseGrouper.result_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    995\u001b[0m codes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreconstructed_codes\n\u001b[1;32m    996\u001b[0m levels \u001b[38;5;241m=\u001b[39m [ping\u001b[38;5;241m.\u001b[39mresult_index \u001b[38;5;28;01mfor\u001b[39;00m ping \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupings]\n\u001b[0;32m--> 997\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMultiIndex\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    998\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnames\u001b[49m\n\u001b[1;32m    999\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/easyFL/lib/python3.8/site-packages/pandas/core/indexes/multi.py:328\u001b[0m, in \u001b[0;36mMultiIndex.__new__\u001b[0;34m(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity)\u001b[0m\n\u001b[1;32m    325\u001b[0m result\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    327\u001b[0m \u001b[38;5;66;03m# we've already validated levels and codes, so shortcut here\u001b[39;00m\n\u001b[0;32m--> 328\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_codes(codes, copy\u001b[38;5;241m=\u001b[39mcopy, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    331\u001b[0m result\u001b[38;5;241m.\u001b[39m_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(levels)\n",
      "File \u001b[0;32m~/miniconda3/envs/easyFL/lib/python3.8/site-packages/pandas/core/indexes/multi.py:834\u001b[0m, in \u001b[0;36mMultiIndex._set_levels\u001b[0;34m(self, levels, level, copy, validate, verify_integrity)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(names):\n\u001b[1;32m    832\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_names(names)\n\u001b[0;32m--> 834\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241m.\u001b[39m_reset_cache()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import bnlearn as bn\n",
    "\n",
    "model = bn.structure_learning.fit(merged_df, methodtype='cs', verbose=0)\n",
    "\n",
    "adj_mtx = np.zeros([len(all_vars), len(all_vars)])\n",
    "for edge in model['dag_edges']:     # type:ignore\n",
    "    source, target = edge\n",
    "    source_id = int(source[1:]) - 1\n",
    "    target_id = int(target[1:]) - 1\n",
    "    adj_mtx[source_id][target_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge, swap_pos\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx)\n",
    "espur = spur_edge(groundtruth, adj_mtx)\n",
    "efals = fals_edge(groundtruth, adj_mtx)\n",
    "emiss = miss_edge(groundtruth, adj_mtx)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invariant Causal Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for variable: X1\tCauses:  []\n",
      "Testing for variable: X2\tCauses:  []\n",
      "Testing for variable: X3\tCauses:  []\n",
      "Testing for variable: X4\tCauses:  []\n",
      "Testing for variable: X5\tCauses:  []\n"
     ]
    }
   ],
   "source": [
    "import causalicp as icp\n",
    "\n",
    "for var_idx in range(len(all_vars)):\n",
    "    print(\"Testing for variable:\", all_vars[var_idx], end=\"\\t\")\n",
    "    result = icp.fit([silo.to_numpy() for silo in silos], var_idx, alpha=0.05, precompute=True, verbose=False, color=False)\n",
    "    print(\"Causes: \", result.accepted_sets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal discovery from nonstationary/heterogeneous data (CDNOD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallearn.search.ConstraintBased.CDNOD import cdnod\n",
    "from baselines.FL_FedCDH.mycausallearn.utils.data_utils import get_cpdag_from_cdnod, get_dag_from_pdag\n",
    "from causallearn.utils.cit import fisherz\n",
    "\n",
    "c_indx = []\n",
    "for i in range(len(silos)):\n",
    "    data = silos[i]\n",
    "    c_indx += [i+1] * len(data)\n",
    "c_indx = np.array(c_indx).reshape(len(silos)*len(silos[0]), 1)\n",
    "\n",
    "cg = cdnod(merged_df.to_numpy(), c_indx, 0.05, fisherz)\n",
    "\n",
    "est_graph = cg.G.graph[0:len(all_vars), 0:len(all_vars)]\n",
    "est_cpdag = get_cpdag_from_cdnod(est_graph) # est_graph[i,j]=-1 & est_graph[j,i]=1  ->  est_graph_cpdag[i,j]=1\n",
    "est_dag_from_pdag = get_dag_from_pdag(est_cpdag) # return a DAG from a PDAG in causaldag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge\n",
    "\n",
    "etrue = true_edge(groundtruth, est_dag_from_pdag)\n",
    "espur = spur_edge(groundtruth, est_dag_from_pdag)\n",
    "efals = fals_edge(groundtruth, est_dag_from_pdag)\n",
    "emiss = miss_edge(groundtruth, est_dag_from_pdag)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Constraint-based Fast Causal Inference (FCI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causallearn.search.ConstraintBased.FCI import fci\n",
    "\n",
    "# default parameters\n",
    "g, edges = fci(merged_df.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for edge in edges:\n",
    "#     print(edge, \"\\t\", edge.get_node1(), edge.get_numerical_endpoint1(), edge.get_node2(), edge.get_numerical_endpoint2())\n",
    "\n",
    "adj_mtx = np.zeros([len(all_vars), len(all_vars)])\n",
    "for edge in edges:     # type:ignore\n",
    "    source_id = int(edge.get_node1().get_name()[1:]) - 1\n",
    "    target_id = int(edge.get_node2().get_name()[1:]) - 1\n",
    "    \n",
    "    if edge.get_numerical_endpoint1() == -1:\n",
    "        adj_mtx[source_id][target_id] = 1\n",
    "    elif edge.get_numerical_endpoint1() != 2:\n",
    "        adj_mtx[target_id][source_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge, swap_pos\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx)\n",
    "espur = spur_edge(groundtruth, adj_mtx)\n",
    "efals = fals_edge(groundtruth, adj_mtx)\n",
    "emiss = miss_edge(groundtruth, adj_mtx)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value-based HillClimb GES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bnlearn as bn\n",
    "\n",
    "model = bn.structure_learning.fit(merged_df, methodtype='hc', verbose=0)\n",
    "adj_mtx = model['adjmat'].to_numpy() * 1.0      # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx.T)\n",
    "espur = spur_edge(groundtruth, adj_mtx.T)\n",
    "efals = fals_edge(groundtruth, adj_mtx.T)\n",
    "emiss = miss_edge(groundtruth, adj_mtx.T)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value-based Treesearch Chow-Liu Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bnlearn as bn\n",
    "# Preprocessing raw dataset\n",
    "dfhot, dfnum = bn.df2onehot(merged_df)\n",
    "\n",
    "# Structure learning\n",
    "model = bn.structure_learning.fit(dfnum, methodtype='cl', verbose=0, root_node='X1')\n",
    "adj_mtx = model['adjmat'].to_numpy() * 1.0      # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx)\n",
    "espur = spur_edge(groundtruth, adj_mtx)\n",
    "efals = fals_edge(groundtruth, adj_mtx)\n",
    "emiss = miss_edge(groundtruth, adj_mtx)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value-based Tree-augmented Naive Bayes (TAN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bnlearn as bn\n",
    "\n",
    "# Structure learning\n",
    "model = bn.structure_learning.fit(merged_df, methodtype='tan', class_node='X1', verbose=0)\n",
    "pruned_model = bn.independence_test(model, merged_df, alpha=0.05, prune=True)\n",
    "\n",
    "adj_mtx = pruned_model['adjmat'].to_numpy() * 1.0      # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx)\n",
    "espur = spur_edge(groundtruth, adj_mtx)\n",
    "efals = fals_edge(groundtruth, adj_mtx)\n",
    "emiss = miss_edge(groundtruth, adj_mtx)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CDT.CAUSALITY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concave penalized Coordinate Descent with reparametrization (CCDR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdt.causality.graph import CCDr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Greedy Interventional Equivalence Search algorithm (GIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdt.causality.graph import GIES\n",
    "import networkx as nx\n",
    "\n",
    "obj = GIES()\n",
    "output = obj.predict(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for e in output.edges:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Structural Agnostic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdt.causality.graph import SAM\n",
    "\n",
    "obj = SAM()\n",
    "output = obj.predict(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Causal Generative Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cdt.causality.graph import CGNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
