{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-0.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-1.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-2.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-3.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-4.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-5.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-6.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-7.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-8.csv\t2500  Instances\t 40 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d40_p0.1/m5_d1.0_n10/silo-9.csv\t2500  Instances\t 40 Variables\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from upgrade import *\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "dataname = \"erdos_renyi/d40_p0.1\"\n",
    "mi = 5      # The number of values a variable can take is ranged in [2, mi-1]\n",
    "di = 1.0      # The dirichlet alpha that controls the data distribution\n",
    "n = 10      # The number of data silos\n",
    "\n",
    "silos = []\n",
    "\n",
    "# folderpath = f\"./data/distributed/erdos_renyi/d20_p0.2/m3_d1_n10\"\n",
    "# groundtruth = np.loadtxt(f\"./data/distributed/erdos_renyi/d20_p0.2/adj.txt\")\n",
    "\n",
    "folderpath = f\"./data/distributed/{dataname}/m{mi}_d{di}_n{n}\"\n",
    "groundtruth = np.loadtxt(f\"./data/distributed/{dataname}/adj.txt\")\n",
    "\n",
    "if not Path(folderpath).exists():\n",
    "    print(\"Folder\", folderpath, \"not exist!\")\n",
    "else:\n",
    "    for file in sorted(os.listdir(folderpath)):\n",
    "        filename = os.path.join(folderpath, file)\n",
    "        silo_data = pd.read_csv(filename)\n",
    "        silos.append(silo_data)\n",
    "        print(\"Loaded file:\", filename, end=\"\\t\")\n",
    "        all_vars = silos[0].columns\n",
    "        print(len(silo_data), \" Instances\\t\", len(all_vars), \"Variables\")\n",
    "\n",
    "all_vars = list(all_vars)\n",
    "merged_df = pd.concat(silos, axis=0)\n",
    "# merged_df['count'] = [1] * len(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.01\n",
    "connectivity = {var: [] for var in all_vars}\n",
    "chisq_obj = CIT(merged_df, \"chisq\")\n",
    "\n",
    "for X in connectivity.keys():\n",
    "    other_vars = list(set(all_vars) - set(connectivity[X]) - set([X]))\n",
    "    for Y in other_vars:\n",
    "        pval = chisq_obj(all_vars.index(X), all_vars.index(Y), []) # type: ignore\n",
    "        if pval <= confidence: # type: ignore\n",
    "            connectivity[X] = list(set(connectivity[X]) | set([Y]))\n",
    "            connectivity[Y] = list(set(connectivity[Y]) | set([X]))\n",
    "\n",
    "# for X, club_X in connectivity.items():\n",
    "#     print(X, club_X, len(club_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X15', 'X32', 'X5', 'X37', 'X3', 'X19', 'X12']\n"
     ]
    }
   ],
   "source": [
    "basis = []\n",
    "ordering = sorted(list(connectivity.keys()), key=lambda item: len(connectivity[item]), reverse=False)\n",
    "while len(ordering):\n",
    "    var = ordering.pop(0)\n",
    "    discard_list = connectivity[var]\n",
    "    ordering = list(set(ordering) - set(discard_list))\n",
    "    ordering = sorted(ordering, key=lambda item: len(list(set(connectivity[item]) - set(discard_list))), reverse=False)\n",
    "    basis.append(var)\n",
    "\n",
    "print(basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_distributions(P0: np.ndarray, num_gen=100, gamma2=0.8):\n",
    "    Ulist = list(np.eye(P0.shape[0]))\n",
    "    # Compute the boundary points\n",
    "    boundaries = []\n",
    "    for i in range(len(Ulist)):\n",
    "        if P0[i]/gamma2 < 1:\n",
    "            alpha_i = 1/(1 - P0[i]) * (1 - P0[i]/(gamma2 + 0.001))\n",
    "            boundary_i = alpha_i * P0 + (1 - alpha_i) * Ulist[i]\n",
    "        else:\n",
    "            boundary_i = Ulist[i]\n",
    "        boundaries.append(boundary_i)\n",
    "    \n",
    "    boundaries = np.stack(boundaries)\n",
    "    w = np.random.uniform(0, 1, (num_gen * 30, len(Ulist)))\n",
    "    w = w/w.sum(axis=1, keepdims=True)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_gen, n_init=\"auto\")\n",
    "    kmeans.fit(w @ boundaries)\n",
    "    res = kmeans.cluster_centers_\n",
    "    \n",
    "    return res\n",
    "    # return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov_blankets = {var: [] for var in all_vars}\n",
    "confidence = 0.01\n",
    "\n",
    "num_env = 100\n",
    "gamma2 = 0.8\n",
    "\n",
    "sample_dis = {var: generate_uniform_distributions(P0=marginal_prob(merged_df, [var]),\n",
    "                                                num_gen=num_env, \n",
    "                                                gamma2=np.power(gamma2, 1./len(basis))) for var in basis}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multivariate_sampling(data: pd.DataFrame, variables: list, sample_dis: dict, instance_index):\n",
    "    remains = deepcopy(variables)\n",
    "    while len(remains):\n",
    "        sampling_var = remains.pop(0)\n",
    "        distribution = sample_dis[sampling_var][instance_index]\n",
    "        _, all_index = univariate_sampling(data, sampling_var, {i: distribution[i] for i in range(distribution.shape[0])})\n",
    "    return all_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GSMB(indexes, confidence=0.01):\n",
    "    data = merged_df.iloc[indexes].reset_index().drop(columns=['index'])\n",
    "    markov_blankets = {}\n",
    "    chisq_obj = CIT(data, \"chisq\") # construct a CIT instance with data and method name\n",
    "    all_var_idx = [i for i in range(len(data.columns))]\n",
    "\n",
    "    for X in all_var_idx:\n",
    "        S = []\n",
    "        # X = 6\n",
    "        prev_length = 0\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            # print(\"==============New cycle==================\")\n",
    "            for Y in list(set(all_var_idx) - set(S) - set([X])):\n",
    "                if Y != X:\n",
    "                    pval = chisq_obj(X, Y, S) # type:ignore\n",
    "                    if pval <= confidence: # type:ignore\n",
    "                        S.append(Y)\n",
    "            \n",
    "            for Y in deepcopy(S):\n",
    "                pval = chisq_obj(X, Y, list(set(S) - set([Y]))) # type:ignore\n",
    "                if pval > confidence: # type:ignore\n",
    "                    S.remove(Y)\n",
    "            \n",
    "            if (len(S) - prev_length == 0) or (count >= 10):\n",
    "                break\n",
    "            else:\n",
    "                prev_length = len(S)\n",
    "\n",
    "        markov_blankets[data.columns[X]] = [data.columns[i] for i in S]\n",
    "    return markov_blankets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Parallel\n",
    "\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# Number of parallel executions\n",
    "num_parallel_executions = num_env\n",
    "silos_index = [multivariate_sampling(merged_df, basis, sample_dis, i) for i in range(num_parallel_executions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "def run_in_parallel(func, args_list, max_parallel_executions):\n",
    "    with ProcessPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "        future_to_arg = {executor.submit(func, arg): arg for arg in args_list}\n",
    "        for future in concurrent.futures.as_completed(future_to_arg):\n",
    "            arg = future_to_arg[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f'{arg} generated an exception: {exc}')\n",
    "    return results\n",
    "\n",
    "# Running F in parallel and storing results\n",
    "results = run_in_parallel(GSMB, silos_index, max_parallel_executions=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "mb = GSMB([i for i in range(len(merged_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X16',\n",
       " 'X36',\n",
       " 'X1',\n",
       " 'X6',\n",
       " 'X14',\n",
       " 'X26',\n",
       " 'X34',\n",
       " 'X24',\n",
       " 'X27',\n",
       " 'X35',\n",
       " 'X37',\n",
       " 'X20',\n",
       " 'X12',\n",
       " 'X19',\n",
       " 'X28']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mb['X2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 1., 0., 0., 1.,\n",
       "       0., 1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groundtruth[all_vars.index('X2')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X37', 'X22', 'X5', 'X30', 'X15'], dtype='<U3')"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_vars)[np.where(groundtruth[all_vars.index('X2')])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "for res in results:\n",
    "    for var, blanket in res.items(): #type:ignore\n",
    "        markov_blankets[var] += blanket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = -1\n",
    "\n",
    "mk_with_freq = {var: [] for var in all_vars}\n",
    "for var in markov_blankets.keys():\n",
    "    mk_with_freq[var] = []\n",
    "    vals, freqs = np.unique(markov_blankets[var], return_counts=True)\n",
    "    # print(\"Variable\", var)\n",
    "    for val, freq in zip(vals, freqs):\n",
    "        # print(f\"\\t[{val}, {freq:>2}]\", end=\"\")\n",
    "        if freq >= int(0.8*num_env):\n",
    "            if val not in mk_with_freq[var]:\n",
    "                mk_with_freq[var].append((val, freq))\n",
    "            # if var not in mk_with_freq[val]:\n",
    "            #     mk_with_freq[val].append((var, freq))\n",
    "\n",
    "for var in markov_blankets.keys():\n",
    "    mk_with_freq[var] = [var for var, freq in sorted(mk_with_freq[var], key=lambda item: item[1], reverse=True)[:max_size]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_viaindexesv2(indexes: list, variable: str, parents: list):\n",
    "    conditional_probs_record = merged_df[parents + [variable]].groupby(parents + [variable]).count().reset_index()\n",
    "    mll_list = []\n",
    "    env = 0\n",
    "    for index in indexes:\n",
    "        vertical_sampled_data = merged_df.iloc[index].reset_index()\n",
    "        vertical_sampled_data = vertical_sampled_data.drop(columns=['index'])\n",
    "        vertical_sampled_data.insert(0, 'count', [1] * len(vertical_sampled_data))\n",
    "        \n",
    "        summary_with_ch = vertical_sampled_data.groupby(parents + [variable])['count'].sum().reset_index()\n",
    "        mll, output = compute_mll(summary_with_ch, parents, env)\n",
    "        conditional_probs_record = conditional_probs_record.merge(output, on=parents + [variable], how='left')\n",
    "        mll_list.append(mll)\n",
    "        env += 1\n",
    "    \n",
    "    mean_mll = np.mean(mll_list)\n",
    "    var_avg = conditional_probs_record.iloc[:, len(parents) + 1:].var(axis=1, skipna=True).mean()\n",
    "    # return var_avg, mean_mll, conditional_probs_record\n",
    "    return var_avg, parents\n",
    "\n",
    "\n",
    "# def compute_weighted_variance_viaindexesv2(indexes: list, variable: str, parents: list):\n",
    "#     variance, _, df = compute_variance_viaindexesv2(indexes, variable, parents)\n",
    "#     if len(parents):\n",
    "#         joint_mat = np.array([df[f'joint_{i}'] for i in range(len(indexes))]).T\n",
    "#         probs_mat = np.array([df[f'probs_{i}'] for i in range(len(indexes))]).T\n",
    "#         probs_mean = []\n",
    "#         for i in range(probs_mat.shape[0]):\n",
    "#             if len(probs_mat[i][~np.isnan(probs_mat[i])]):\n",
    "#                 probs_mean.append(np.mean(probs_mat[i][~np.isnan(probs_mat[i])]).item())\n",
    "#             else:\n",
    "#                 probs_mean.append(0)\n",
    "                \n",
    "#         probs_mean = np.expand_dims(np.array(probs_mean), 1)\n",
    "#         prod = joint_mat * (probs_mat - probs_mean)**2\n",
    "#         return np.mean(prod[~np.isnan(prod)]), parents\n",
    "#     else:\n",
    "#         return variance, parents\n",
    "    \n",
    "def F_wrapper(args):\n",
    "    # return compute_weighted_variance_viaindexesv2(*args)\n",
    "    return compute_weighted_variance_viasilos(*args)\n",
    "\n",
    "# Function to execute F in parallel with limited concurrency\n",
    "def run_in_parallel2(func, args_list, max_parallel_executions):\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=max_parallel_executions) as executor:\n",
    "        future_to_arg = {executor.submit(func, args): args for args in args_list}\n",
    "        for future in concurrent.futures.as_completed(future_to_arg):\n",
    "            arg = future_to_arg[future]\n",
    "            try:\n",
    "                result = future.result()\n",
    "                results.append(result)\n",
    "            except Exception as exc:\n",
    "                print(f'{arg} generated an exception: {exc}')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.625"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean([len(mb) for var, mb in mk_with_freq.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import compute_variance_viasilos\n",
    "\n",
    "def compute_weighted_variance_viasilos(silos, variable: str, parents: list, verbose=False):\n",
    "    variance, _, df = compute_variance_viasilos(silos, variable, parents, verbose=verbose)\n",
    "    # if len(parents):\n",
    "    #     joint_mat = np.array([df[f'joint_{i}'] for i in range(len(silos))]).T\n",
    "    #     probs_mat = np.array([df[f'probs_{i}'] for i in range(len(silos))]).T\n",
    "    #     probs_mean = np.array([np.mean(probs_mat[i][~np.isnan(probs_mat[i])], keepdims=True) for i in range(probs_mat.shape[0])])\n",
    "    #     prod = joint_mat * (probs_mat - probs_mean)**2\n",
    "    #     return np.mean(prod[~np.isnan(prod)]), parents\n",
    "    # else:\n",
    "    return variance, parents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable: X6 Markov: ['X2', 'X4', 'X5', 'X7']\n",
      "|Combs| =  4\n",
      "\t ['X5', 'X2', 'X7'] 0.06942196443184559\n",
      "|Combs| =  3\n",
      "\t ['X5', 'X2'] 0.050942485905525345\n",
      "|Combs| =  2\n",
      "\t ['X5'] 0.028738502536979866\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "count_num_comb = 0\n",
    "# for anchor_var in tqdm(mk_with_freq.keys(), leave=False):\n",
    "anchor_var = 'X6'\n",
    "print(\"Variable:\", anchor_var, \"Markov:\", mk_with_freq[anchor_var])\n",
    "markov_variables = list(set(mk_with_freq[anchor_var]))\n",
    "# if len(markov_variables) < 1:\n",
    "#     continue\n",
    "\n",
    "lowest_variance = 1e2\n",
    "best_comb = []\n",
    "\n",
    "# prev_variance, _ = compute_variance_viaindexesv2(silos_index, anchor_var, markov_variables)\n",
    "prev_variance, _ = compute_weighted_variance_viasilos(silos, anchor_var, markov_variables)\n",
    "while len(markov_variables) - 1 > 0:\n",
    "    results = []\n",
    "    all_comb = [list(comb) for comb in list(combinations(markov_variables, len(markov_variables) - 1))]\n",
    "    print(\"|Combs| = \", len(all_comb))\n",
    "    count_num_comb += len(all_comb)\n",
    "    # inputs = [[silos_index, anchor_var, comb] for comb in all_comb]\n",
    "    # results = run_in_parallel2(F_wrapper, inputs, max_parallel_executions=32)\n",
    "    inputs = [[silos, anchor_var, comb] for comb in all_comb]\n",
    "    results = run_in_parallel2(F_wrapper, inputs, max_parallel_executions=32)\n",
    "    lowest_variance, best_comb = sorted(results, key=lambda item: item[0])[0]\n",
    "    if lowest_variance > prev_variance:\n",
    "        print(lowest_variance, prev_variance)\n",
    "        break\n",
    "    else:\n",
    "        markov_variables = best_comb\n",
    "        prev_variance = lowest_variance\n",
    "        print(\"\\t\", best_comb, lowest_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05099419553661425\n"
     ]
    }
   ],
   "source": [
    "prev_variance, _ = compute_variance_viaindexesv2(silos_index, 'X6', ['X4', 'X2'])\n",
    "print(prev_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "variances = [res[0] for res in results]\n",
    "comb = [res[1] for res in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.53569927e-09, 6.15631991e-09, 6.05825613e-09, 7.02660611e-09,\n",
       "       6.47463676e-09, 5.97368715e-09, 6.02515827e-09, 5.92700422e-09,\n",
       "       5.92976833e-09, 5.93499713e-09, 5.98445469e-09, 6.46912431e-09,\n",
       "       6.17036838e-09, 6.75444089e-09, 6.20805770e-09, 6.09449987e-09,\n",
       "       7.72815589e-09, 8.63803407e-09, 5.67242709e-09, 6.03744145e-09,\n",
       "       6.29401716e-09, 1.83184485e-08])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_var = 'X2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('X1', 50),\n",
       " ('X10', 49),\n",
       " ('X11', 46),\n",
       " ('X15', 42),\n",
       " ('X16', 50),\n",
       " ('X18', 50),\n",
       " ('X22', 49),\n",
       " ('X23', 45),\n",
       " ('X24', 44),\n",
       " ('X26', 50),\n",
       " ('X29', 47),\n",
       " ('X3', 45),\n",
       " ('X31', 49),\n",
       " ('X33', 50),\n",
       " ('X35', 50),\n",
       " ('X36', 48),\n",
       " ('X37', 40),\n",
       " ('X38', 50),\n",
       " ('X39', 50),\n",
       " ('X4', 50),\n",
       " ('X40', 48),\n",
       " ('X5', 50),\n",
       " ('X9', 50)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_with_freq[anchor_var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['X18', 'X24'], dtype='<U3')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(all_vars)[np.where(groundtruth[all_vars.index(anchor_var)])[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [2:19:13<34:48, 2088.25s/it]\n"
     ]
    }
   ],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "potential_parents = {var: [] for var in all_vars}\n",
    "children = {var: [] for var in all_vars}\n",
    "invariance_hardcap = 0.001\n",
    "max_size = 8\n",
    "\n",
    "repeat = 5\n",
    "adj_record = []\n",
    "\n",
    "\n",
    "for _ in tqdm(range(repeat), leave=False):\n",
    "    for anchor_var in tqdm(mk_with_freq.keys(), leave=False):\n",
    "        markov_variables = list(set(mk_with_freq[anchor_var]) - set(children[anchor_var]))\n",
    "        if len(markov_variables) < 1:\n",
    "            continue\n",
    "        \n",
    "        lowest_variance = 1e2\n",
    "        best_comb = []\n",
    "        \n",
    "        prev_variance, _ = compute_weighted_variance_viaindexesv2(silos_index, anchor_var, markov_variables)\n",
    "        while len(markov_variables) - 1 > 0:\n",
    "            results = []\n",
    "            all_comb = [list(comb) for comb in list(combinations(markov_variables, len(markov_variables) - 1))]\n",
    "            inputs = [[silos_index, anchor_var, comb] for comb in all_comb]\n",
    "            results = run_in_parallel2(F_wrapper, inputs, max_parallel_executions=64)\n",
    "            lowest_variance, best_comb = sorted(results, key=lambda item: item[0])[0]\n",
    "            if lowest_variance > prev_variance:\n",
    "                break\n",
    "            else:\n",
    "                markov_variables = best_comb\n",
    "                prev_variance = lowest_variance\n",
    "        potential_parents[anchor_var] = (best_comb, lowest_variance) # type:ignore\n",
    "        \n",
    "    adj_mtx = np.zeros([len(all_vars), len(all_vars)])\n",
    "    for var in potential_parents.keys():\n",
    "        if len(potential_parents[var]):\n",
    "            parents, invariance = potential_parents[var]\n",
    "            var_id = int(var[1:]) - 1\n",
    "            for pa in parents:\n",
    "                pa_id = int(pa[1:]) - 1\n",
    "                if adj_mtx[var_id][pa_id] == 0:\n",
    "                    adj_mtx[pa_id][var_id] = invariance\n",
    "                elif adj_mtx[var_id][pa_id] > adj_mtx[pa_id][var_id]:\n",
    "                    adj_mtx[pa_id][var_id] = invariance\n",
    "                    adj_mtx[var_id][pa_id] = 0\n",
    "\n",
    "    for i in range(len(all_vars)):\n",
    "        children[f'X{i+1}'] = []\n",
    "        for j in range(len(all_vars)):\n",
    "            if adj_mtx[i][j] > 0:\n",
    "                children[f'X{i+1}'].append(f'X{j+1}')\n",
    "                \n",
    "    adj_record.append(adj_mtx)\n",
    "    if len(adj_record) >= 2:\n",
    "        if np.sum(adj_record[-1] - adj_record[-2]) == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 147 67 23\n",
      "21 126 69 23\n",
      "21 129 68 24\n",
      "21 130 68 24\n",
      "21 130 68 24\n"
     ]
    }
   ],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge, swap_pos\n",
    "\n",
    "for fin_adjmtx in adj_record:\n",
    "    # fin_adjmtx = adj_record[-1]\n",
    "\n",
    "    etrue = true_edge(groundtruth, fin_adjmtx)\n",
    "    espur = spur_edge(groundtruth, fin_adjmtx)\n",
    "    efals = fals_edge(groundtruth, fin_adjmtx)\n",
    "    emiss = miss_edge(groundtruth, fin_adjmtx)\n",
    "\n",
    "    # print(etrue)\n",
    "    print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "fin_adjmtx = adj_record[0]\n",
    "\n",
    "for i in range(fin_adjmtx.shape[0]):\n",
    "    for j in range(fin_adjmtx.shape[1]):\n",
    "        if fin_adjmtx[i][j] > 0:\n",
    "            G.add_edge(f\"X{i+1}\", f\"X{j+1}\", weight=np.round(1/fin_adjmtx[i][j],2))\n",
    "            # print(\"Here add edge\", f\"X{i+1}\", f\"X{j+1}\")\n",
    "    G.add_node(f\"X{i+1}\")\n",
    "    \n",
    "\n",
    "etrue = true_edge(groundtruth, fin_adjmtx)\n",
    "espur = spur_edge(groundtruth, fin_adjmtx)\n",
    "efals = fals_edge(groundtruth, fin_adjmtx)\n",
    "emiss = miss_edge(groundtruth, fin_adjmtx)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))\n",
    "\n",
    "pos = nx.shell_layout(G)\n",
    "pos = swap_pos(pos, 'X4', 'X3')\n",
    "# pos = swap_pos(pos, 'X3', 'X5')\n",
    "# pos = swap_pos(pos, 'X10', 'X5')\n",
    "# pos = swap_pos(pos, 'X10', 'X8')\n",
    "# pos = swap_pos(pos, 'X8', 'X3')\n",
    "# pos = swap_pos(pos, 'X6', 'X5')\n",
    "# pos = swap_pos(pos, 'X1', 'X2')\n",
    "# pos = swap_pos(pos, 'X11', 'X8')\n",
    "# pos = swap_pos(pos, 'X7', 'X19')\n",
    "# pos = swap_pos(pos, 'X20', 'X7')\n",
    "# pos = swap_pos(pos, 'X18', 'X16')\n",
    "\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=400, node_color=\"#1f78b4\")\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G, pos, edgelist=espur, width=2, arrowstyle='->', arrowsize=20, edge_color=\"orange\", label=\"Spurious Edges\")\n",
    "nx.draw_networkx_edges(G, pos, edgelist=emiss, width=2, arrowstyle='->', arrowsize=20, edge_color=\"purple\", label=\"Missing Edges\")\n",
    "nx.draw_networkx_edges(G, pos, edgelist=efals, width=2, arrowstyle='->', arrowsize=20, edge_color=\"red\", label=\"Anti-Causal Edges\")\n",
    "nx.draw_networkx_edges(G, pos, edgelist=etrue, width=2, arrowstyle='->', arrowsize=20, edge_color=\"green\", label=\"Causal Edges\")\n",
    "\n",
    "# node labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=12, font_family=\"sans-serif\", font_color='white')\n",
    "\n",
    "# edge weight labels\n",
    "# edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "# plt.box()\n",
    "plt.title(dataname.upper())\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"res/asia.plot.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for anchor_var in mk_with_freq.keys():\n",
    "anchor_var = 'X6'\n",
    "print(\"anchor_var:\", anchor_var, end=\"\")\n",
    "print(mk_with_freq[anchor_var])\n",
    "markov_variables = mk_with_freq[anchor_var]\n",
    "if len(markov_variables) < 1:\n",
    "    pass\n",
    "else:\n",
    "    lowest_variance = 1e2\n",
    "    best_comb = []\n",
    "\n",
    "    print(\"anchor var:\", anchor_var, \"Len(markov) = \", len(markov_variables))\n",
    "    for l in range(1, len(markov_variables) + 1):\n",
    "        for comb in list(combinations(markov_variables, l)):\n",
    "            comb_variance = compute_weighted_variance_viasilos(silos, anchor_var, list(comb)) # type:ignore\n",
    "            print(\"\\tdoing comb\", comb, \"variance:\", comb_variance)\n",
    "            if comb_variance < lowest_variance and comb_variance < invariance_hardcap:\n",
    "                lowest_variance = comb_variance\n",
    "                best_comb = list(comb)\n",
    "\n",
    "    print(\"\\tParents:\", best_comb, \"\\tVariance:\", lowest_variance)\n",
    "    potential_parents[anchor_var] = (best_comb, lowest_variance) # type:ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parents: ['X4'] \tVariance: 1.1418060575318796e-06"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
