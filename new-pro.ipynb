{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Env: Minh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-0.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-1.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-2.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-3.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-4.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-5.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-6.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-7.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-8.csv\t2500  Instances 300 Variables\n",
      "Loaded file: ./data/distributed/erdos_renyi/d300_p0.01/m5_d1.0_n10/silo-9.csv\t2500  Instances 300 Variables\n",
      "Num edges: 716.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils.upgrade import *\n",
    "from causallearn.utils.cit import CIT\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "# dataname = \"munin1\"\n",
    "dataname = \"erdos_renyi/d300_p0.01\"\n",
    "mi = 5      # The number of values a variable can take is ranged in [2, mi-1]\n",
    "di = 1.0      # The dirichlet alpha that controls the data distribution\n",
    "n = 10      # The number of data silos\n",
    "\n",
    "silos = []\n",
    "\n",
    "folderpath = f\"./data/distributed/{dataname}/m{mi}_d{di}_n{n}\"\n",
    "groundtruth = np.loadtxt(f\"./data/distributed/{dataname}/adj.txt\")\n",
    "\n",
    "if not Path(folderpath).exists():\n",
    "    print(\"Folder\", folderpath, \"not exist!\")\n",
    "else:\n",
    "    for file in sorted(os.listdir(folderpath)):\n",
    "        filename = os.path.join(folderpath, file)\n",
    "        silo_data = pd.read_csv(filename)\n",
    "        silos.append(silo_data)\n",
    "        print(\"Loaded file:\", filename, end=\"\\t\")\n",
    "        print(len(silo_data), \" Instances\", len(silo_data.columns), \"Variables\")\n",
    "\n",
    "merged_df = pd.concat(silos[:-1], axis=0)\n",
    "merged_df = merged_df.reindex(sorted(merged_df.columns, key=lambda item: int(item[1:])), axis=1)\n",
    "all_vars = list(merged_df.columns)\n",
    "\n",
    "print(\"Num edges:\", np.sum(groundtruth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_markov_blanket(adj_matrix, var_idx):\n",
    "    parents = np.where(adj_matrix[:, var_idx])[0].tolist()\n",
    "    children = np.where(adj_matrix[var_idx])[0].tolist()\n",
    "    \n",
    "    spouses = set()\n",
    "    for c in children:\n",
    "        for sp in np.where(adj_matrix[:, c])[0]:\n",
    "            spouses.add(sp)\n",
    "    \n",
    "    pa_sp = list(set(parents)&spouses - set(parents))\n",
    "    ch_sp = list(set(children)&spouses - set(children))\n",
    "    spouses = list(spouses - set(pa_sp) - set(ch_sp))\n",
    "    \n",
    "    return parents, pa_sp, spouses, ch_sp, children\n",
    "\n",
    "\n",
    "def to_list(all_vars, mb_idx_list):\n",
    "    return [all_vars[i] for i in mb_idx_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = 0.05\n",
    "connectivity = {var: [] for var in all_vars}\n",
    "chisq_obj = CIT(merged_df, \"chisq\")\n",
    "\n",
    "for X in connectivity.keys():\n",
    "    other_vars = list(set(all_vars) - set(connectivity[X]) - set([X]))\n",
    "    for Y in other_vars:\n",
    "        pval = chisq_obj(all_vars.index(X), all_vars.index(Y), []) # type: ignore\n",
    "        if pval <= confidence: # type: ignore\n",
    "            connectivity[X] = list(set(connectivity[X]) | set([Y]))\n",
    "            connectivity[Y] = list(set(connectivity[Y]) | set([X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connectivity['X2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "basis = []\n",
    "ordering = sorted(all_vars, key=lambda item: len(connectivity[item]), reverse=False)\n",
    "\n",
    "while len(ordering):\n",
    "    x = ordering.pop(0)\n",
    "    discard_vars = connectivity[x]\n",
    "    ordering = sorted(list(set(ordering) - set(discard_vars)), \n",
    "                    key=lambda item: len(list(set(connectivity[item]) - set(discard_vars))), reverse=False)\n",
    "    basis.append(x)\n",
    "    \n",
    "# basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "def GSMB(indexes, confidence=0.01):\n",
    "    data = merged_df.iloc[indexes].reset_index().drop(columns=['index'])\n",
    "    chisq_obj = CIT(data, \"chisq\") # construct a CIT instance with data and method name\n",
    "    all_var_idx = [i for i in range(len(data.columns))]\n",
    "    markov_blankets_idx = {i: [] for i in range(len(data.columns))}\n",
    "\n",
    "    for X in all_var_idx:\n",
    "        S = []\n",
    "        prev_length = 0\n",
    "        count = 0\n",
    "        while True:\n",
    "            count += 1\n",
    "            # print(\"==============New cycle==================\")\n",
    "            for Y in list(set(all_var_idx) - set(S) - set([X])):\n",
    "                if Y != X:\n",
    "                    pval = chisq_obj(X, Y, S) # type:ignore\n",
    "                    if pval <= confidence: # type:ignore\n",
    "                        S.append(Y)\n",
    "            \n",
    "            for Y in deepcopy(S):\n",
    "                pval = chisq_obj(X, Y, list(set(S) - set([Y]))) # type:ignore\n",
    "                if pval > confidence: # type:ignore\n",
    "                    S.remove(Y)\n",
    "            \n",
    "            if (len(S) - prev_length == 0) or (count > 2):\n",
    "                break\n",
    "            else:\n",
    "                prev_length = len(S)\n",
    "        markov_blankets_idx[X] = list(set(markov_blankets_idx[X])|set(S))\n",
    "    \n",
    "    markov_blankets = {var: [] for var in all_vars}\n",
    "    for idx, mb_idxes in markov_blankets_idx.items():\n",
    "        var = all_vars[idx]\n",
    "        markov_blankets[var] = [all_vars[i] for i in mb_idxes]\n",
    "    \n",
    "    return markov_blankets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TMB_activated = 1\n",
    "markov_blankets = {var: [] for var in all_vars}\n",
    "\n",
    "if TMB_activated:\n",
    "    for var in markov_blankets.keys():\n",
    "        pa, pa_sp, sp, ch_sp, ch = true_markov_blanket(groundtruth, int(var[1:]) - 1)\n",
    "        markov_blankets[var] = list(set(to_list(all_vars, pa + pa_sp + sp + ch_sp + ch)) - set([var]))\n",
    "else:\n",
    "    markov_blankets = GSMB([i for i in range(len(merged_df))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_uniform_distributions(P0: np.ndarray, num_gen=100, gamma2=0.8):\n",
    "    Ulist = list(np.eye(P0.shape[0]))\n",
    "    # Compute the boundary points\n",
    "    boundaries = []\n",
    "    for i in range(len(Ulist)):\n",
    "        if P0[i]/gamma2 < 1:\n",
    "            alpha_i = 1/(1 - P0[i]) * (1 - P0[i]/(gamma2 + 0.001))\n",
    "            boundary_i = alpha_i * P0 + (1 - alpha_i) * Ulist[i]\n",
    "        else:\n",
    "            boundary_i = Ulist[i]\n",
    "        boundaries.append(boundary_i)\n",
    "    \n",
    "    boundaries = np.stack(boundaries)\n",
    "    w = np.concatenate([np.random.dirichlet([alpha/2] * len(Ulist), size=num_gen) for alpha in range(1, 10)])\n",
    "    \n",
    "    kmeans = KMeans(n_clusters=num_gen, n_init=\"auto\")\n",
    "    kmeans.fit(w @ boundaries)\n",
    "    res = kmeans.cluster_centers_\n",
    "    \n",
    "    return res\n",
    "\n",
    "def multivariate_sampling(data: pd.DataFrame, variables: list, sample_dis: dict, instance_index):\n",
    "    remains = deepcopy(variables)\n",
    "    while len(remains):\n",
    "        sampling_var = remains.pop(0)\n",
    "        distribution = sample_dis[sampling_var][instance_index]\n",
    "        _, all_index = univariate_sampling(data, sampling_var, {i: distribution[i] for i in range(distribution.shape[0])})\n",
    "    return all_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnested(input: list):\n",
    "    if len(input) == 1:\n",
    "        if isinstance(input[0], list):\n",
    "            return unnested(input[0])\n",
    "        else:\n",
    "            return input\n",
    "    else:\n",
    "        nested_loc = [i for i in range(len(input)) if isinstance(input[i], list)]\n",
    "        while len(nested_loc):\n",
    "            i = nested_loc.pop(0)\n",
    "            input += [*input[i]]\n",
    "            input.pop(i)\n",
    "            nested_loc = [i for i in range(len(input)) if isinstance(input[i], list)]\n",
    "        return list(set(input))\n",
    "\n",
    "# test_input = ['X66']\n",
    "# unnested(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffers = {}\n",
    "visited = []\n",
    "def recursive_conn(neighbors):   \n",
    "    output = []\n",
    "    if len(neighbors) <= 1:\n",
    "        output = [neighbors]\n",
    "    else:\n",
    "        for i in neighbors:\n",
    "            key = sorted(list(set(neighbors)&set(markov_blankets[i])))\n",
    "            if tuple(key) in buffers.keys():\n",
    "                # print(\"Here in\", i)\n",
    "                res_i = [i] + buffers[tuple(key)]\n",
    "            else:\n",
    "                # print(\"Here recur\", i)\n",
    "                val = recursive_conn(key)\n",
    "                buffers[tuple(key)] = val\n",
    "                res_i = [i] + val\n",
    "\n",
    "            visit_key = tuple(sorted(unnested(deepcopy(res_i))))\n",
    "            if visit_key not in visited:\n",
    "                # print(visit_key)\n",
    "                output.append(res_i)\n",
    "                visited.append(visit_key)\n",
    "    return output\n",
    "\n",
    "\n",
    "def unfold(input):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "      input: [var, var, ..., [var, ...], [var, ...]]\n",
    "\n",
    "    that has a number of non-list element and a number of list element\n",
    "    \"\"\"\n",
    "    cut_index = 0\n",
    "    while cut_index < len(input):\n",
    "      cut_index += 1\n",
    "      if isinstance(input[cut_index], list):\n",
    "        break\n",
    "\n",
    "    out = []\n",
    "    for i in range(cut_index, len(input)):\n",
    "      out.append([*input[:cut_index], *input[i]])\n",
    "    \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = recursive_conn(markov_blankets['X57'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['X52', ['X151', ['X185', ['X189']]], ['X250', []], ['X5', []]],\n",
       " ['X274', ['X223', []], ['X263', ['X5']]],\n",
       " ['X151', ['X152', []], ['X185', ['X189', ['X52']]]],\n",
       " ['X185', ['X151', ['X189', ['X52']]]],\n",
       " ['X50', ['X149', ['X152', ['X170', ['X187']]]]],\n",
       " ['X5', ['X263', ['X274']], ['X52', []]],\n",
       " ['X33', []],\n",
       " ['X250', ['X130', ['X223']]],\n",
       " ['X263', ['X274', ['X5']]],\n",
       " ['X187',\n",
       "  ['X149', ['X152', ['X170', ['X50']]]],\n",
       "  ['X170', ['X149', ['X152', ['X50']]], ['X189', []]],\n",
       "  ['X189', ['X170']]],\n",
       " ['X223', ['X130', ['X250']], ['X274', []]],\n",
       " ['X152', ['X149', ['X170', ['X187', ['X50']]]], ['X151', []]]]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "recursive_outputs = {}\n",
    "\n",
    "for anchor_var in all_vars:\n",
    "    buffers.clear()\n",
    "    visited.clear()\n",
    "    recursive_outputs[anchor_var] = recursive_conn(deepcopy(markov_blankets[anchor_var]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removes_irrelevant(df, var, plausible_set, confidence=0.01):\n",
    "    subdata = df[[var, *plausible_set]]\n",
    "    all_var = list(subdata.columns)\n",
    "    all_var_idx = [i for i in range(len(all_var))]\n",
    "    chisq_obj = CIT(subdata, 'chisq')\n",
    "    \n",
    "    X = all_var.index(var)\n",
    "    S = []\n",
    "    prev_length = 0\n",
    "    count = 0\n",
    "    while True:\n",
    "        count += 1\n",
    "        for Y in deepcopy(S):\n",
    "            pval = chisq_obj(X, Y, list(set(S) - set([Y]))) # type:ignore\n",
    "            if pval > confidence: # type:ignore\n",
    "                S.remove(Y)\n",
    "                \n",
    "        for Y in list(set(all_var_idx) - set(S) - set([X])):\n",
    "            if Y != X:\n",
    "                pval = chisq_obj(X, Y, S) # type:ignore\n",
    "                if pval <= confidence: # type:ignore\n",
    "                    S.append(Y)\n",
    "                    \n",
    "        if (len(S) - prev_length == 0) or (count > 10):\n",
    "            break\n",
    "        else:\n",
    "            prev_length = len(S)\n",
    "        \n",
    "    return [all_var[i] for i in S]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential_parents = {}\n",
    "for anchor_var in markov_blankets.keys():\n",
    "    recursive_output = recursive_outputs[anchor_var]\n",
    "    final_output = set()\n",
    "    for i in range(len(recursive_output)):\n",
    "        test_case = deepcopy(recursive_output[i])\n",
    "        unique_elements = set()\n",
    "        if len(test_case) <= 1:\n",
    "            unique_elements.add(tuple(test_case))\n",
    "        else:\n",
    "            first_element = test_case.pop(0)\n",
    "            while len(test_case):\n",
    "                examine_group = test_case.pop(0)\n",
    "                if len(examine_group) and not isinstance(examine_group[0], list) and isinstance(examine_group[-1], list):\n",
    "                    test_case += [*unfold(examine_group)]\n",
    "                else:\n",
    "                    unique_elements.add(tuple(sorted(examine_group + [first_element])))\n",
    "                \n",
    "        final_output = final_output|unique_elements\n",
    "    potential_parents[anchor_var] = [removes_irrelevant(merged_df, anchor_var, j, 0.05) for j in final_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "groundtruth_dict = {}\n",
    "for var in all_vars:\n",
    "    var_id = all_vars.index(var)\n",
    "    pa, pa_sp, sp, ch_sp, ch = true_markov_blanket(groundtruth, var_id)\n",
    "    groundtruth_dict[var] = [all_vars[i] for i in pa + pa_sp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X277 False ['X16', 'X58', 'X66', 'X72', 'X108', 'X112', 'X137', 'X182', 'X190', 'X261', 'X273', 'X295']\n"
     ]
    }
   ],
   "source": [
    "for var in all_vars:\n",
    "    checked = []\n",
    "    for po in potential_parents[var]:\n",
    "        checked.append(set(groundtruth_dict[var]) & set(po) == set(groundtruth_dict[var]))\n",
    "    \n",
    "    if True in checked:\n",
    "        # print(var, \"True\")\n",
    "        pass\n",
    "    else:\n",
    "        print(var, \"False\", groundtruth_dict[var])\n",
    "        \n",
    "        # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_variance_viaindexesv2(indexes: list, variable: str, parents: list):\n",
    "    conditional_probs_record = merged_df[parents + [variable]].groupby(parents + [variable]).count().reset_index()\n",
    "    mll_list = []\n",
    "    env = 0\n",
    "    for index in indexes:\n",
    "        vertical_sampled_data = merged_df.iloc[index].reset_index()\n",
    "        vertical_sampled_data = vertical_sampled_data.drop(columns=['index'])\n",
    "        vertical_sampled_data.insert(0, 'count', [1] * len(vertical_sampled_data))\n",
    "        \n",
    "        summary_with_ch = vertical_sampled_data.groupby(parents + [variable])['count'].sum().reset_index()\n",
    "        mll, output = compute_mll(summary_with_ch, parents, env)\n",
    "        conditional_probs_record = conditional_probs_record.merge(output, on=parents + [variable], how='left')\n",
    "        mll_list.append(mll)\n",
    "        env += 1\n",
    "    \n",
    "    mean_mll = np.mean(mll_list)\n",
    "    var_avg = conditional_probs_record.iloc[:, len(parents) + 1:].var(axis=1, skipna=True).mean()\n",
    "    return var_avg, mean_mll, conditional_probs_record\n",
    "\n",
    "\n",
    "def compute_weighted_variance_viaindexesv2(indexes: list, variable: str, parents: list):\n",
    "    variance, _, df = compute_variance_viaindexesv2(indexes, variable, parents)\n",
    "    if len(parents):\n",
    "        joint_mat = np.array([df[f'joint_{i}'] for i in range(len(indexes))]).T\n",
    "        probs_mat = np.array([df[f'probs_{i}'] for i in range(len(indexes))]).T\n",
    "        probs_mean = []\n",
    "        for i in range(probs_mat.shape[0]):\n",
    "            if len(probs_mat[i][~np.isnan(probs_mat[i])]):\n",
    "                probs_mean.append(np.mean(probs_mat[i][~np.isnan(probs_mat[i])]).item())\n",
    "            else:\n",
    "                probs_mean.append(0)\n",
    "                \n",
    "        probs_mean = np.expand_dims(np.array(probs_mean), 1)\n",
    "        # joint_mat = joint_mat.shape[1] * joint_mat/joint_mat.sum(axis=1, keepdims=True)\n",
    "        prod = joint_mat * (probs_mat - probs_mean)**2\n",
    "        return np.power(np.mean(prod[~np.isnan(prod)]), 0.5), parents\n",
    "    else:\n",
    "        return variance, parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 2 -- Given the leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def individual_causal_search(var, silos_index):\n",
    "    record = {}\n",
    "    for mb_var in markov_blankets[var]:\n",
    "        variance, _ = compute_weighted_variance_viaindexesv2(silos_index, var, [mb_var])\n",
    "        record[tuple([mb_var])] = variance\n",
    "    return {var: record}\n",
    "\n",
    "\n",
    "# Function to execute F in parallel\n",
    "def execute_in_parallel(args_list: List[Tuple]):\n",
    "    with Pool() as pool:\n",
    "        # Map the function F to the arguments in parallel\n",
    "        results = pool.starmap(individual_causal_search, args_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaves = ['X8']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_env = 10\n",
    "gamma2 = 0.5\n",
    "\n",
    "sample_dis = {x: generate_uniform_distributions(P0=marginal_prob(merged_df, [x]),\n",
    "                                                num_gen=num_env, \n",
    "                                                gamma2=np.power(gamma2, 1./len(leaves))) for x in leaves}\n",
    "silos_index = [multivariate_sampling(merged_df, leaves, sample_dis, i) for i in range(num_env)]\n",
    "\n",
    "inputs = [(var, silos_index) for var in markov_blankets.keys()]\n",
    "outputs = execute_in_parallel(inputs)\n",
    "\n",
    "results = tuple()\n",
    "for out_dict in outputs:\n",
    "    results += tuple(out_dict.items())\n",
    "\n",
    "results = dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mtx = np.ones([len(all_vars), len(all_vars)])\n",
    "\n",
    "for var in results.keys(): #type:ignore\n",
    "    if len(results[var].items()):\n",
    "        var_id = all_vars.index(var)\n",
    "        best_comb, best_variance = min(results[var].items(), key=lambda item: item[1])\n",
    "        # print(var, best_comb, best_variance)\n",
    "        for parent in best_comb:\n",
    "            pa_id = all_vars.index(parent)\n",
    "            if best_variance < weighted_mtx[var_id][pa_id]:\n",
    "                weighted_mtx[pa_id][var_id] = best_variance\n",
    "                weighted_mtx[var_id][pa_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mtx[weighted_mtx == 1] = 0\n",
    "adj_mtx = (weighted_mtx > 0) * 1\n",
    "adj_mtx = adj_mtx.T\n",
    "# adj_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import true_edge, spur_edge, fals_edge, miss_edge, swap_pos\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx)\n",
    "espur = spur_edge(groundtruth, adj_mtx)\n",
    "efals = fals_edge(groundtruth, adj_mtx)\n",
    "emiss = miss_edge(groundtruth, adj_mtx)\n",
    "\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sources_idx = np.array([i for i in range(len(all_vars)) if np.sum(adj_mtx[:, i]) == 0])\n",
    "sources = np.array(all_vars)[sources_idx].tolist()\n",
    "sources = list(set(sources) - set(basis))\n",
    "sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Version 1 -- Given the sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sources_idx = [i for i in range(groundtruth.shape[0]) if np.sum(groundtruth[:,i]) == 0]\n",
    "# sources = np.array(all_vars)[sources_idx].tolist()\n",
    "# sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from typing import List, Tuple\n",
    "\n",
    "\n",
    "def individual_causal_searchv2(var, silos_index):\n",
    "    buffers = {}\n",
    "    print(var, len(potential_parents[var]))\n",
    "    for group in potential_parents[var]:\n",
    "        conn_group = list(set(connectivity[var])&set(group))\n",
    "        # print(\"Applied Connectivity:\", group, \"-->\", conn_group)\n",
    "        cleaned_group = removes_irrelevant(merged_df, var, conn_group)\n",
    "        # print(\"Group:\", conn_group, \"-->\", cleaned_group)\n",
    "        if len(cleaned_group):\n",
    "            # variance, _, _ = compute_variance_viaindexesv2(silos_index, var, cleaned_group)\n",
    "            variance, _ = compute_weighted_variance_viaindexesv2(silos_index, var, cleaned_group)\n",
    "            buffers[tuple(cleaned_group)] = variance\n",
    "    return {var: buffers}\n",
    "\n",
    "\n",
    "# Function to execute F in parallel\n",
    "def execute_in_parallel(args_list: List[Tuple]):\n",
    "    with Pool() as pool:\n",
    "        # Map the function F to the arguments in parallel\n",
    "        results = pool.starmap(individual_causal_searchv2, args_list)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_env = 10\n",
    "gamma2 = 0.5\n",
    "\n",
    "sample_dis = {x: generate_uniform_distributions(P0=marginal_prob(merged_df, [x]),\n",
    "                                                num_gen=num_env, \n",
    "                                                gamma2=np.power(gamma2, 1./len(basis))) for x in basis}\n",
    "silos_index = [multivariate_sampling(merged_df, basis, sample_dis, i) for i in range(num_env)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 1\n",
      "X3 2\n",
      "X5 5\n",
      "X2 2\n",
      "X7 3\n",
      "X9 3\n",
      "X11 2\n",
      "X4 2\n",
      "X13 14\n",
      "X15 2\n",
      "X17 1\n",
      "X12 4\n",
      "X8 4\n",
      "X19 2\n",
      "X6 7\n",
      "X10 2\n",
      "X18 2\n",
      "X21 10\n",
      "X16 6\n",
      "X23 4\n",
      "X25 1\n",
      "X20 3\n",
      "X27 2\n",
      "X26 2\n",
      "X29 1\n",
      "X31 3\n",
      "X30 1\n",
      "X33 3\n",
      "X35 3\n",
      "X28 3\n",
      "X37 1\n",
      "X24 6\n",
      "X39 1\n",
      "X41 2\n",
      "X38 2\n",
      "X43 3\n",
      "X40 3\n",
      "X32 3\n",
      "X45 4\n",
      "X34 8\n",
      "X47 8\n",
      "X36 1\n",
      "X22 3\n",
      "X49 5\n",
      "X42 3\n",
      "X51 5\n",
      "X53 19\n",
      "X55 1\n",
      "X44 4\n",
      "X57 10\n",
      "X46 6\n",
      "X59 4\n",
      "X56 1\n",
      "X61 5\n",
      "X63 6\n",
      "X14 7\n",
      "X65 3\n",
      "X67 2\n",
      "X52 14\n",
      "X69 1\n",
      "X71 3\n",
      "X68 1\n",
      "X50 5\n",
      "X73 3\n",
      "X70 1\n",
      "X75 1\n",
      "X64 7\n",
      "X60 2\n",
      "X77 8\n",
      "X48 1\n",
      "X66 11\n",
      "X62 8\n",
      "X79 2\n",
      "X76 10\n",
      "X72 3\n",
      "X81 7\n",
      "X74 3\n",
      "X83 6\n",
      "X85 1\n",
      "X87 4\n",
      "X58 8\n",
      "X80 2\n",
      "X89 5\n",
      "X86 4\n",
      "X91 2\n",
      "X93 6\n",
      "X95 3\n",
      "X97 4\n",
      "X99 2\n",
      "X82 9\n",
      "X78 1\n",
      "X92 5\n",
      "X101 1\n",
      "X103 2\n",
      "X100 1\n",
      "X54 23\n",
      "X105 10\n",
      "X102 2\n",
      "X88 14\n",
      "X90 8\n",
      "X107 1\n",
      "X109 1\n",
      "X96 1\n",
      "X111 4\n",
      "X84 12\n",
      "X108 2\n",
      "X113 4\n",
      "X104 12\n",
      "X110 8\n",
      "X115 2\n",
      "X94 4\n",
      "X117 4\n",
      "X98 11\n",
      "X119 12\n",
      "X121 5\n",
      "X123 4\n",
      "X116 2\n",
      "X125 11\n",
      "X127 8\n",
      "X129 1\n",
      "X112 3\n",
      "X131 7\n",
      "X133 2\n",
      "X118 2\n",
      "X130 5\n",
      "X114 2\n",
      "X135 2\n",
      "X137 3\n",
      "X106 11\n",
      "X139 2\n",
      "X141 1\n",
      "X134 8\n",
      "X143 14\n",
      "X136 10\n",
      "X145 6\n",
      "X142 3\n",
      "X122 7\n",
      "X147 2\n",
      "X149 2\n",
      "X124 3\n",
      "X140 4\n",
      "X151 4\n",
      "X153 2\n",
      "X155 2\n",
      "X138 2\n",
      "X148 5\n",
      "X157 1\n",
      "X159 2\n",
      "X150 1\n",
      "X161 3\n",
      "X158 2\n",
      "X132 6\n",
      "X163 13\n",
      "X154 3\n",
      "X165 4\n",
      "X156 2\n",
      "X128 1\n",
      "X167 4\n",
      "X152 8\n",
      "X160 2\n",
      "X169 8\n",
      "X171 3\n",
      "X126 4\n",
      "X146 6\n",
      "X173 5\n",
      "X175 21\n",
      "X162 1\n",
      "X120 2\n",
      "X177 3\n",
      "X166 7\n",
      "X179 10\n",
      "\n",
      "10 X181X183 4\n",
      "X168 5\n",
      "X172 3\n",
      "X144 6\n",
      "X185 5\n",
      "X170 6\n",
      "X187 8\n",
      "X189 10\n",
      "X178 4\n",
      "X191 5\n",
      "X164 11\n",
      "X193 3\n",
      "X174 6\n",
      "X195 2\n",
      "X197 7\n",
      "X186 1\n",
      "X199 8\n",
      "X184 2\n",
      "X201 12\n",
      "X203 5\n",
      "X196 3\n",
      "X205 5\n",
      "X194 1\n",
      "X207 8\n",
      "X192 8\n",
      "X209 1\n",
      "X198 1\n",
      "X211 9\n",
      "X213 3\n",
      "X210 2\n",
      "X182 11\n",
      "X215 8\n",
      "X217 7\n",
      "X206 3\n",
      "X190 7\n",
      "X219 2\n",
      "X188 6\n",
      "X200 2\n",
      "X180 4\n",
      "X221 5\n",
      "X223 6\n",
      "X204 1\n",
      "X176 9\n",
      "X214 10\n",
      "X225 5\n",
      "X227 7\n",
      "X208 1\n",
      "X220 3\n",
      "X229 4\n",
      "X231 1\n",
      "X233 3\n",
      "X235 4\n",
      "X232 6\n",
      "X237 2\n",
      "X239 1\n",
      "X216 3\n",
      "X241 5\n",
      "X243 2\n",
      "X222 2\n",
      "X212 8\n",
      "X245 21\n",
      "X224 4\n",
      "X238 12\n",
      "X240 7\n",
      "X234 10\n",
      "X247 5\n",
      "X218 1\n",
      "X249 7\n",
      "X226 8\n",
      "X202 2\n",
      "X251 6\n",
      "X230 4\n",
      "X253 13\n",
      "X236 2\n",
      "X228 1\n",
      "X255 4\n",
      "X257 8\n",
      "X259 3\n",
      "X244 3\n",
      "X261 7\n",
      "X242 9\n",
      "X263 5\n",
      "X250 11\n",
      "X265 8\n",
      "X267 1\n",
      "X269 5\n",
      "X256 2\n",
      "X271 6\n",
      "X268 7\n",
      "X248 2\n",
      "X273 7\n",
      "X252 2\n",
      "X260 6\n",
      "X275 1\n",
      "X277 1\n",
      "X278 2\n",
      "X262 3\n",
      "X276 3\n",
      "X279 3\n",
      "X270 3\n",
      "X281 3\n",
      "X258 2\n",
      "X283 6\n",
      "X264 6\n",
      "X285 4\n",
      "X287 4\n",
      "X289 5\n",
      "X280 2\n",
      "X291 2\n",
      "X274 13\n",
      "X293 2\n",
      "X282 3\n",
      "X266 3\n",
      "X272 3\n",
      "X295 12\n",
      "X254 3\n",
      "X286 4\n",
      "X297 9\n",
      "X299 3\n",
      "X292 3\n",
      "X288 1\n",
      "X294 5\n",
      "X284 2\n",
      "X290 6\n",
      "X246 1\n",
      "X300 8\n",
      "X298 2\n",
      "X296 5\n"
     ]
    }
   ],
   "source": [
    "inputs = [(var, silos_index) for var in markov_blankets.keys()]\n",
    "outputs = execute_in_parallel(inputs)\n",
    "\n",
    "results = tuple()\n",
    "for out_dict in outputs:\n",
    "    results += tuple(out_dict.items())\n",
    "\n",
    "results = dict(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mtx = np.ones([len(all_vars), len(all_vars)])\n",
    "\n",
    "for var in results.keys(): #type:ignore\n",
    "    var_id = all_vars.index(var)\n",
    "    if len(results[var].items()):\n",
    "        best_comb, best_variance = min(results[var].items(), key=lambda item: item[1])\n",
    "        # print(var, best_comb, best_variance)\n",
    "        \n",
    "        for parent in best_comb:\n",
    "            pa_id = all_vars.index(parent)\n",
    "            if best_variance < weighted_mtx[var_id][pa_id]:\n",
    "                weighted_mtx[pa_id][var_id] = best_variance\n",
    "                weighted_mtx[var_id][pa_id] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardcap_invariance = 1e-3\n",
    "weighted_mtx[weighted_mtx > hardcap_invariance] = 0\n",
    "adj_mtx = (weighted_mtx > 0) * 1\n",
    "# adj_mtx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283 30 293 140\n"
     ]
    }
   ],
   "source": [
    "from utils.plot_utils import true_edge, spur_edge, fals_edge, miss_edge, swap_pos\n",
    "\n",
    "etrue = true_edge(groundtruth, adj_mtx)\n",
    "espur = spur_edge(groundtruth, adj_mtx)\n",
    "efals = fals_edge(groundtruth, adj_mtx)\n",
    "emiss = miss_edge(groundtruth, adj_mtx)\n",
    "\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Left-out code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_var = 'X2'\n",
    "pa, pa_sp, sp, ch_sp, ch = true_markov_blanket(groundtruth, int(inv_var[1:]) - 1)\n",
    "print(\"Pa:\", to_list(all_vars, pa))\n",
    "print(\"Pa-Sp:\", to_list(all_vars, pa_sp))\n",
    "print(\"Sp:\", to_list(all_vars, sp))\n",
    "print(\"Ch-Sp:\", to_list(all_vars, ch_sp))\n",
    "print(\"Ch:\", to_list(all_vars, ch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in results.keys(): #type:ignore\n",
    "    best_comb, best_variance = min(results[var].items(), key=lambda item: item[1])\n",
    "    print(var, \"\\t\", groundtruth_dict[var], \"\\t\", best_comb, \"\\t\", best_variance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot -- Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "G = nx.DiGraph()\n",
    "\n",
    "fin_adjmtx = adj_mtx\n",
    "\n",
    "for i in range(fin_adjmtx.shape[0]):\n",
    "    for j in range(fin_adjmtx.shape[1]):\n",
    "        if fin_adjmtx[i][j] > 0:\n",
    "            G.add_edge(f\"X{i+1}\", f\"X{j+1}\", weight=np.round(1/fin_adjmtx[i][j],2))\n",
    "            # print(\"Here add edge\", f\"X{i+1}\", f\"X{j+1}\")\n",
    "    G.add_node(f\"X{i+1}\")\n",
    "    \n",
    "\n",
    "etrue = true_edge(groundtruth, fin_adjmtx)\n",
    "espur = spur_edge(groundtruth, fin_adjmtx)\n",
    "efals = fals_edge(groundtruth, fin_adjmtx)\n",
    "emiss = miss_edge(groundtruth, fin_adjmtx)\n",
    "\n",
    "# print(etrue)\n",
    "print(len(etrue), len(espur), len(emiss), len(efals))\n",
    "\n",
    "pos = nx.shell_layout(G)\n",
    "pos = swap_pos(pos, 'X4', 'X3')\n",
    "# pos = swap_pos(pos, 'X3', 'X5')\n",
    "\n",
    "# nodes\n",
    "nx.draw_networkx_nodes(G, pos, node_size=400, node_color=\"#1f78b4\")\n",
    "\n",
    "# edges\n",
    "nx.draw_networkx_edges(G, pos, edgelist=espur, width=2, arrowstyle='->', arrowsize=20, edge_color=\"orange\", label=\"Spurious Edges\")\n",
    "nx.draw_networkx_edges(G, pos, edgelist=emiss, width=2, arrowstyle='->', arrowsize=20, edge_color=\"purple\", label=\"Missing Edges\")\n",
    "nx.draw_networkx_edges(G, pos, edgelist=efals, width=2, arrowstyle='->', arrowsize=20, edge_color=\"red\", label=\"Anti-Causal Edges\")\n",
    "nx.draw_networkx_edges(G, pos, edgelist=etrue, width=2, arrowstyle='->', arrowsize=20, edge_color=\"green\", label=\"Causal Edges\")\n",
    "\n",
    "# node labels\n",
    "nx.draw_networkx_labels(G, pos, font_size=12, font_family=\"sans-serif\", font_color='white')\n",
    "\n",
    "# edge weight labels\n",
    "# edge_labels = nx.get_edge_attributes(G, \"weight\")\n",
    "# nx.draw_networkx_edge_labels(G, pos, edge_labels)\n",
    "\n",
    "ax = plt.gca()\n",
    "ax.margins(0.08)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "# plt.box()\n",
    "plt.title(dataname.upper())\n",
    "# plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(\"res/asia.plot.svg\", format=\"svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easyFL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
